MTPR from untrained_mtps/06.mtp, Database: cfg_train_eq/Al_train_100_eq_2000.cfg
validation set: cfg_test/Al_test_1000.cfg
Random initialization of radial coefficients
Rescaling...
   scaling = 0.833333333333333, condition number = 507.461880531569
   scaling = 0.909090909090909, condition number = 553.589934777236
   scaling = 1, condition number = 608.943774755749
   scaling = 1.1, condition number = 669.833468197748
   scaling = 1.2, condition number = 730.723534772508
Rescaling to 0.833333333333333... done
Rescaling...
   scaling = 0.694444444444445, condition number = 422.895921915831
   scaling = 0.757575757575758, condition number = 461.334844901039
   scaling = 0.833333333333333, condition number = 507.462143916133
   scaling = 0.916666666666667, condition number = 558.202735127272
   scaling = 1, condition number = 608.943774756293
Rescaling to 0.694444444444445... done
Rescaling...
   scaling = 0.578703703703704, condition number = 352.426240908539
   scaling = 0.631313131313131, condition number = 384.457591008233
   scaling = 0.694444444444445, condition number = 422.895921915831
   scaling = 0.763888888888889, condition number = 465.178764021042
   scaling = 0.833333333333333, condition number = 507.462143916133
Rescaling to 0.578703703703704... done
Rescaling...
   scaling = 0.482253086419753, condition number = 293.704116518417
   scaling = 0.526094276094276, condition number = 320.395602257949
   scaling = 0.578703703703704, condition number = 352.426240908539
   scaling = 0.636574074074074, condition number = 387.660758629349
   scaling = 0.694444444444445, condition number = 422.895921915831
Rescaling to 0.482253086419753... done
Rescaling...
   scaling = 0.401877572016461, condition number = 244.772156060448
   scaling = 0.43841189674523, condition number = 267.013487597862
   scaling = 0.482253086419753, condition number = 293.704116518417
   scaling = 0.530478395061729, condition number = 323.064789477597
   scaling = 0.578703703703704, condition number = 352.426240908539
Rescaling to 0.401877572016461... done
Rescaling...
   scaling = 0.334897976680384, condition number = 203.999314497035
   scaling = 0.365343247287692, condition number = 222.53185853447
   scaling = 0.401877572016461, condition number = 244.772156060448
   scaling = 0.442065329218107, condition number = 269.237667348875
   scaling = 0.482253086419753, condition number = 293.704116535801
Rescaling to 0.334897976680384... done
Rescaling...
   scaling = 0.279081647233653, condition number = 170.026530819107
   scaling = 0.304452706073077, condition number = 185.468020303989
   scaling = 0.334897976680384, condition number = 203.999314497035
   scaling = 0.368387774348423, condition number = 224.385169374593
   scaling = 0.401877572016461, condition number = 244.772156060448
Rescaling to 0.279081647233653... done
Rescaling...
   scaling = 0.232568039361378, condition number = 141.721438032309
   scaling = 0.25371058839423, condition number = 154.586557240583
   scaling = 0.279081647233653, condition number = 170.026530819107
   scaling = 0.306989811957019, condition number = 187.012237713556
   scaling = 0.334897976680384, condition number = 203.999314497035
Rescaling to 0.232568039361378... done
Rescaling...
   scaling = 0.193806699467815, condition number = 118.140635194186
   scaling = 0.211425490328525, condition number = 128.858165415998
   scaling = 0.232568039361378, condition number = 141.721438032309
   scaling = 0.255824843297516, condition number = 155.873152464285
   scaling = 0.279081647233653, condition number = 170.026530819107
Rescaling to 0.193806699467815... done
Rescaling...
   scaling = 0.161505582889846, condition number = 98.4982721661068
   scaling = 0.176187908607104, condition number = 107.425367969314
   scaling = 0.193806699467815, condition number = 118.140635194186
   scaling = 0.213187369414596, condition number = 129.930020433487
   scaling = 0.232568039361378, condition number = 141.721438032309
Rescaling to 0.161505582889846... done
Rescaling...
   scaling = 0.134587985741538, condition number = 82.139909651708
   scaling = 0.146823257172587, condition number = 89.573973907294
   scaling = 0.161505582889846, condition number = 98.4982721661068
   scaling = 0.17765614117883, condition number = 108.318203262407
   scaling = 0.193806699467815, condition number = 118.140635198205
Rescaling to 0.134587985741538... done
Rescaling...
   scaling = 0.112156654784615, condition number = 68.5208002025933
   scaling = 0.122352714310489, condition number = 74.7093452007808
   scaling = 0.134587985741538, condition number = 82.139909651708
   scaling = 0.148046784315692, condition number = 90.3175368101912
   scaling = 0.161505582889846, condition number = 98.4982721661068
Rescaling to 0.112156654784615... done
Rescaling...
   scaling = 0.0934638789871793, condition number = 49.7968007743894
   scaling = 0.101960595258741, condition number = 59.214723421661
   scaling = 0.112156654784615, condition number = 68.5208002025933
   scaling = 0.123372320263077, condition number = 75.3283972293849
   scaling = 0.134587985741538, condition number = 82.139909651708
Rescaling to 0.0934638789871793... done
Rescaling...
   scaling = 0.0778865658226494, condition number = 34.6598167553734
   scaling = 0.0849671627156175, condition number = 41.1986017051252
   scaling = 0.0934638789871793, condition number = 49.7968007743894
   scaling = 0.102810266885897, condition number = 60.2015933690744
   scaling = 0.112156654784615, condition number = 68.5208002025933
Rescaling to 0.0778865658226494... done
Rescaling...
   scaling = 0.0649054715188745, condition number = 24.1520113437437
   scaling = 0.0708059689296813, condition number = 28.6907446789791
   scaling = 0.0778865658226494, condition number = 34.6598167553734
   scaling = 0.0856752224049144, condition number = 41.8838030554312
   scaling = 0.0934638789871793, condition number = 49.796800761683
Rescaling to 0.0649054715188745... done
Rescaling...
   scaling = 0.0540878929323954, condition number = 16.8607298816997
   scaling = 0.0590049741080677, condition number = 20.0095852650338
   scaling = 0.0649054715188745, condition number = 24.1520113437437
   scaling = 0.071396018670762, condition number = 29.1663993648563
   scaling = 0.0778865658226494, condition number = 34.65981677736
Rescaling to 0.0540878929323954... done
Rescaling...
   scaling = 0.0450732441103295, condition number = 11.8058138815514
   scaling = 0.0491708117567231, condition number = 13.98807357959
   scaling = 0.0540878929323954, condition number = 16.8607298816997
   scaling = 0.059496682225635, condition number = 20.3396377039601
   scaling = 0.0649054715188745, condition number = 24.1520113437437
Rescaling to 0.0450732441103295... done
Rescaling...
   scaling = 0.0375610367586079, condition number = 8.30785923444936
   scaling = 0.0409756764639359, condition number = 9.81680673062346
   scaling = 0.0450732441103295, condition number = 11.8058138815514
   scaling = 0.0495805685213625, condition number = 14.2168933329171
   scaling = 0.0540878929323954, condition number = 16.8607298766011
Rescaling to 0.0375610367586079... done
Rescaling...
   scaling = 0.0313008639655066, condition number = 5.89689385760685
   scaling = 0.03414639705328, condition number = 6.93523912049351
   scaling = 0.0375610367586079, condition number = 8.30785923444936
   scaling = 0.0413171404344687, condition number = 9.97514836882729
   scaling = 0.0450732441103295, condition number = 11.8058138815514
Rescaling to 0.0313008639655066... done
Rescaling...
   scaling = 0.0260840533045889, condition number = 4.24910933851544
   scaling = 0.0284553308777333, condition number = 4.95630661815903
   scaling = 0.0313008639655066, condition number = 5.89689385760685
   scaling = 0.0344309503620573, condition number = 7.04437617654536
   scaling = 0.0375610367586079, condition number = 8.30785923444936
Rescaling to 0.0260840533045889... done
Rescaling...
   scaling = 0.0217367110871574, condition number = 3.1428242618165
   scaling = 0.0237127757314444, condition number = 3.61414615513206
   scaling = 0.0260840533045889, condition number = 4.24910933851544
   scaling = 0.0286924586350477, condition number = 5.0308964754329
   scaling = 0.0313008639655066, condition number = 5.89689385760685
Rescaling to 0.0217367110871574... done
Rescaling...
   scaling = 0.0181139259059645, condition number = 2.6774902796296
   scaling = 0.0197606464428703, condition number = 2.75805381466967
   scaling = 0.0217367110871574, condition number = 3.1428242618165
   scaling = 0.0239103821958731, condition number = 3.66422067997829
   scaling = 0.0260840533045889, condition number = 4.24910933851544
Rescaling to 0.0181139259059645... done
Rescaling...
   scaling = 0.0150949382549704, condition number = 2.64478148364431
   scaling = 0.0164672053690586, condition number = 2.63597337087984
   scaling = 0.0181139259059645, condition number = 2.6774902796296
   scaling = 0.0199253184965609, condition number = 2.76773972063754
   scaling = 0.0217367110871574, condition number = 3.14282425943687
Rescaling to 0.0164672053690586... done
Rescaling...
   scaling = 0.0137226711408822, condition number = 2.38077917333199
   scaling = 0.0149701866991442, condition number = 2.64810876870374
   scaling = 0.0164672053690586, condition number = 2.63597337087984
   scaling = 0.0181139259059645, condition number = 2.6774902796296
   scaling = 0.0197606464428703, condition number = 2.75805381466967
Rescaling to 0.0137226711408822... done
Rescaling...
   scaling = 0.0114355592840685, condition number = 2.33254604772213
   scaling = 0.0124751555826202, condition number = 2.06263382057367
   scaling = 0.0137226711408822, condition number = 2.38077917333199
   scaling = 0.0150949382549704, condition number = 2.64478148364431
   scaling = 0.0164672053690586, condition number = 2.63597337087984
Rescaling to 0.0124751555826202... done
Rescaling...
   scaling = 0.0103959629855168, condition number = 2.79948578086772
   scaling = 0.0113410505296547, condition number = 2.36809135164
   scaling = 0.0124751555826202, condition number = 2.06263382057367
   scaling = 0.0137226711408822, condition number = 2.38077917333199
   scaling = 0.0149701866991442, condition number = 2.64810876870374
Rescaling to 0.0124751555826202... done
Pre-training started
MTPR parallel training started
BFGS iter 0: f=0.00445508
BFGS iter 1: f=0.00444554
BFGS iter 2: f=0.00441608
BFGS iter 3: f=0.0041966
BFGS iter 4: f=0.00398171
BFGS iter 5: f=0.00390938
BFGS iter 6: f=0.0038599
BFGS iter 7: f=0.00375387
BFGS iter 8: f=0.00370044
BFGS iter 9: f=0.00366461
BFGS iter 10: f=0.00363706
BFGS iter 11: f=0.00358752
BFGS iter 12: f=0.00354976
BFGS iter 13: f=0.00348042
BFGS iter 14: f=0.00343958
BFGS iter 15: f=0.00341783
BFGS iter 16: f=0.00340416
BFGS iter 17: f=0.00339687
BFGS iter 18: f=0.00338181
BFGS iter 19: f=0.00333597
BFGS iter 20: f=0.00331551
BFGS iter 21: f=0.00330671
BFGS iter 22: f=0.00330218
BFGS iter 23: f=0.00328604
BFGS iter 24: f=0.0032695
BFGS iter 25: f=0.00326111
BFGS iter 26: f=0.00324472
BFGS iter 27: f=0.00323423
BFGS iter 28: f=0.00321778
BFGS iter 29: f=0.00320611
BFGS iter 30: f=0.00319769
BFGS iter 31: f=0.00318306
BFGS iter 32: f=0.0031563
BFGS iter 33: f=0.00314105
BFGS iter 34: f=0.00311823
BFGS iter 35: f=0.00310625
BFGS iter 36: f=0.00310087
BFGS iter 37: f=0.00309253
BFGS iter 38: f=0.00308663
BFGS iter 39: f=0.00306838
BFGS iter 40: f=0.00306386
BFGS iter 41: f=0.00306026
BFGS iter 42: f=0.00305473
BFGS iter 43: f=0.0030481
BFGS iter 44: f=0.00303823
BFGS iter 45: f=0.00303075
BFGS iter 46: f=0.00302143
BFGS iter 47: f=0.00301474
BFGS iter 48: f=0.00300772
BFGS iter 49: f=0.00300108
BFGS iter 50: f=0.00296526
BFGS iter 51: f=0.00296148
BFGS iter 52: f=0.00295774
BFGS iter 53: f=0.00295463
BFGS iter 54: f=0.00294978
BFGS iter 55: f=0.00293953
BFGS iter 56: f=0.00293271
BFGS iter 57: f=0.00292238
BFGS iter 58: f=0.00291196
BFGS iter 59: f=0.00290743
BFGS iter 60: f=0.00290534
BFGS iter 61: f=0.00290364
BFGS iter 62: f=0.00290137
BFGS iter 63: f=0.00290002
BFGS iter 64: f=0.00289853
BFGS iter 65: f=0.00289831
BFGS iter 66: f=0.00289813
BFGS iter 67: f=0.00289779
BFGS iter 68: f=0.00289744
BFGS iter 69: f=0.00289706
BFGS iter 70: f=0.00289391
BFGS iter 71: f=0.00289084
BFGS iter 72: f=0.00288586
BFGS iter 73: f=0.00288297
BFGS iter 74: f=0.00288099
step limit reached
MTPR training ended
Rescaling...
   scaling = 0.0103959629855168, condition number = 8.94892462713282
   scaling = 0.0113410505296547, condition number = 6.98402182266852
   scaling = 0.0124751555826202, condition number = 5.3485328900633
   scaling = 0.0137226711408822, condition number = 4.12565116025789
   scaling = 0.0149701866991442, condition number = 3.28494578600511
Rescaling to 0.0149701866991442... done
Rescaling...
   scaling = 0.0124751555826202, condition number = 5.3485328900633
   scaling = 0.0136092606355856, condition number = 4.21843621353905
   scaling = 0.0149701866991442, condition number = 3.28494578600511
   scaling = 0.0164672053690586, condition number = 2.59614139550716
   scaling = 0.017964224038973, condition number = 2.48585493218874
Rescaling to 0.017964224038973... done
Rescaling...
   scaling = 0.0149701866991442, condition number = 3.28494579109174
   scaling = 0.0163311127627028, condition number = 2.64795637892926
   scaling = 0.017964224038973, condition number = 2.48585493218874
   scaling = 0.0197606464428703, condition number = 2.48630859034994
   scaling = 0.0215570688467676, condition number = 2.55617385183041
Rescaling to 0.017964224038973... done
Pre-training ended
BFGS iterations count set to 1000
BFGS convergence tolerance set to 0.001
Energy weight: 1
Force weight: 0.01
Stress weight: 0.001
MTPR parallel training started
BFGS iter 0: f=0.00283218
BFGS iter 1: f=0.00283217
BFGS iter 2: f=0.0028303
BFGS iter 3: f=0.0028301
BFGS iter 4: f=0.00282981
BFGS iter 5: f=0.00282954
BFGS iter 6: f=0.00282881
BFGS iter 7: f=0.00282858
BFGS iter 8: f=0.00282831
BFGS iter 9: f=0.00282787
BFGS iter 10: f=0.00282776
BFGS iter 11: f=0.00282768
BFGS iter 12: f=0.00282745
BFGS iter 13: f=0.00282521
BFGS iter 14: f=0.00282516
BFGS iter 15: f=0.00282514
BFGS iter 16: f=0.00282514
BFGS iter 17: f=0.0028251
BFGS iter 18: f=0.00282496
BFGS iter 19: f=0.0028243
BFGS iter 20: f=0.00282379
BFGS iter 21: f=0.00282357
BFGS iter 22: f=0.00282351
BFGS iter 23: f=0.00282344
BFGS iter 24: f=0.00282341
BFGS iter 25: f=0.00282336
BFGS iter 26: f=0.002823
BFGS iter 27: f=0.00281976
BFGS iter 28: f=0.00281422
BFGS iter 29: f=0.00281253
BFGS iter 30: f=0.00281144
BFGS iter 31: f=0.00280919
BFGS iter 32: f=0.0028074
BFGS iter 33: f=0.00280668
BFGS iter 34: f=0.00280634
BFGS iter 35: f=0.002806
BFGS iter 36: f=0.00280592
BFGS iter 37: f=0.00280587
BFGS iter 38: f=0.00280583
BFGS iter 39: f=0.00280568
BFGS iter 40: f=0.00280459
BFGS iter 41: f=0.00280341
BFGS iter 42: f=0.00280244
BFGS iter 43: f=0.00280205
BFGS iter 44: f=0.00280184
BFGS iter 45: f=0.00280176
BFGS iter 46: f=0.00280172
BFGS iter 47: f=0.00280164
BFGS iter 48: f=0.00280134
BFGS iter 49: f=0.00280128
BFGS iter 50: f=0.00279652
BFGS iter 51: f=0.00279629
BFGS iter 52: f=0.00279625
BFGS iter 53: f=0.0027962
BFGS iter 54: f=0.00279607
BFGS iter 55: f=0.00279591
BFGS iter 56: f=0.00279584
BFGS iter 57: f=0.0027958
BFGS iter 58: f=0.00279578
BFGS iter 59: f=0.00279572
BFGS iter 60: f=0.00279566
BFGS iter 61: f=0.00279547
BFGS iter 62: f=0.00279532
BFGS iter 63: f=0.00279525
BFGS iter 64: f=0.00279522
BFGS iter 65: f=0.00279514
BFGS iter 66: f=0.00279463
BFGS iter 67: f=0.00279417
BFGS iter 68: f=0.0027937
BFGS iter 69: f=0.00279312
BFGS iter 70: f=0.00279263
BFGS iter 71: f=0.00279195
BFGS iter 72: f=0.00279029
BFGS iter 73: f=0.00278925
BFGS iter 74: f=0.00278497
BFGS iter 75: f=0.00278299
BFGS iter 76: f=0.00278248
BFGS iter 77: f=0.00278142
BFGS iter 78: f=0.00278124
BFGS iter 79: f=0.00278116
BFGS iter 80: f=0.0027811
BFGS iter 81: f=0.00278096
BFGS iter 82: f=0.00277927
BFGS iter 83: f=0.00277912
BFGS iter 84: f=0.00277871
BFGS iter 85: f=0.00277862
BFGS iter 86: f=0.00277859
BFGS iter 87: f=0.00277854
BFGS iter 88: f=0.00277849
BFGS iter 89: f=0.00277846
BFGS iter 90: f=0.00277844
BFGS iter 91: f=0.00277843
BFGS iter 92: f=0.00277843
BFGS iter 93: f=0.00277789
BFGS iter 94: f=0.00277695
BFGS iter 95: f=0.00277604
BFGS iter 96: f=0.00277484
BFGS iter 97: f=0.0027742
BFGS iter 98: f=0.00277358
BFGS iter 99: f=0.00277309
BFGS iter 100: f=0.00277178
BFGS iter 101: f=0.00277168
BFGS iter 102: f=0.00277155
BFGS iter 103: f=0.00277131
BFGS iter 104: f=0.00277096
BFGS iter 105: f=0.00277078
BFGS iter 106: f=0.00277064
BFGS iter 107: f=0.00277056
BFGS iter 108: f=0.00277039
BFGS iter 109: f=0.00277025
BFGS iter 110: f=0.00276999
BFGS iter 111: f=0.00276983
BFGS iter 112: f=0.00276975
BFGS iter 113: f=0.00276971
BFGS iter 114: f=0.00276966
BFGS iter 115: f=0.00276947
BFGS iter 116: f=0.00276874
BFGS iter 117: f=0.00276814
BFGS iter 118: f=0.00276782
BFGS iter 119: f=0.00276777
BFGS iter 120: f=0.00276776
BFGS iter 121: f=0.00276775
BFGS iter 122: f=0.00276774
BFGS iter 123: f=0.00276771
BFGS iter 124: f=0.00276768
BFGS iter 125: f=0.00276759
BFGS iter 126: f=0.00276693
BFGS iter 127: f=0.00276464
BFGS iter 128: f=0.00276347
BFGS iter 129: f=0.00276325
BFGS iter 130: f=0.00276305
BFGS iter 131: f=0.00276287
BFGS iter 132: f=0.00276265
BFGS iter 133: f=0.00276214
BFGS iter 134: f=0.00276199
BFGS iter 135: f=0.00276178
BFGS iter 136: f=0.00276171
BFGS iter 137: f=0.00276166
BFGS iter 138: f=0.00276162
BFGS iter 139: f=0.0027616
BFGS iter 140: f=0.00276159
BFGS iter 141: f=0.00276159
BFGS iter 142: f=0.00276158
BFGS iter 143: f=0.00276158
BFGS iter 144: f=0.00276158
BFGS iter 145: f=0.00276158
BFGS iter 146: f=0.00276154
BFGS iter 147: f=0.00276129
BFGS iter 148: f=0.00276056
BFGS iter 149: f=0.00275916
BFGS iter 150: f=0.00275867
BFGS iter 151: f=0.00275866
BFGS iter 152: f=0.0027586
BFGS iter 153: f=0.00275854
BFGS iter 154: f=0.0027583
BFGS iter 155: f=0.00275809
BFGS iter 156: f=0.00275552
BFGS iter 157: f=0.00275432
BFGS iter 158: f=0.00275376
BFGS iter 159: f=0.00275352
BFGS iter 160: f=0.00275321
BFGS iter 161: f=0.002753
BFGS iter 162: f=0.00275284
BFGS iter 163: f=0.00275277
BFGS iter 164: f=0.00275275
BFGS iter 165: f=0.00275274
BFGS iter 166: f=0.00275274
BFGS iter 167: f=0.00275273
BFGS iter 168: f=0.00275219
BFGS iter 169: f=0.00275158
BFGS iter 170: f=0.00275121
BFGS iter 171: f=0.00275094
BFGS iter 172: f=0.00275077
BFGS iter 173: f=0.0027507
BFGS iter 174: f=0.00275064
BFGS iter 175: f=0.00275059
BFGS iter 176: f=0.00275055
BFGS iter 177: f=0.00275051
BFGS iter 178: f=0.00275048
BFGS iter 179: f=0.00275046
BFGS iter 180: f=0.00275044
BFGS iter 181: f=0.00275043
BFGS iter 182: f=0.00275041
BFGS iter 183: f=0.00275035
BFGS iter 184: f=0.00275019
BFGS iter 185: f=0.00275009
BFGS iter 186: f=0.00274998
BFGS iter 187: f=0.00274994
BFGS iter 188: f=0.00274992
BFGS iter 189: f=0.00274992
BFGS iter 190: f=0.00274991
BFGS iter 191: f=0.00274991
BFGS iter 192: f=0.0027499
BFGS iter 193: f=0.0027499
BFGS iter 194: f=0.00274987
BFGS iter 195: f=0.00274976
BFGS iter 196: f=0.002749
BFGS iter 197: f=0.00274612
BFGS iter 198: f=0.00274059
BFGS iter 199: f=0.00273633
BFGS iter 200: f=0.00273353
BFGS iter 201: f=0.00273351
BFGS iter 202: f=0.00273335
BFGS iter 203: f=0.00273294
BFGS iter 204: f=0.00273157
BFGS iter 205: f=0.00273015
BFGS iter 206: f=0.00272879
BFGS iter 207: f=0.00272756
BFGS iter 208: f=0.0027262
BFGS iter 209: f=0.00272459
BFGS iter 210: f=0.00272302
BFGS iter 211: f=0.00272163
BFGS iter 212: f=0.00272022
BFGS iter 213: f=0.00271898
BFGS iter 214: f=0.002718
BFGS iter 215: f=0.00271728
BFGS iter 216: f=0.00271691
BFGS iter 217: f=0.00271654
BFGS iter 218: f=0.00271589
BFGS iter 219: f=0.00271413
BFGS iter 220: f=0.00271359
BFGS iter 221: f=0.00271336
BFGS iter 222: f=0.00271323
BFGS iter 223: f=0.00271314
BFGS iter 224: f=0.0027131
BFGS iter 225: f=0.00271306
BFGS iter 226: f=0.0027121
BFGS iter 227: f=0.00270981
BFGS iter 228: f=0.00270859
BFGS iter 229: f=0.00270825
BFGS iter 230: f=0.00270804
BFGS iter 231: f=0.00270769
BFGS iter 232: f=0.00270717
BFGS iter 233: f=0.00270647
BFGS iter 234: f=0.00270605
BFGS iter 235: f=0.0027059
BFGS iter 236: f=0.00270545
BFGS iter 237: f=0.00270521
BFGS iter 238: f=0.00270495
BFGS iter 239: f=0.00270481
BFGS iter 240: f=0.00270471
BFGS iter 241: f=0.00270468
BFGS iter 242: f=0.00270466
BFGS iter 243: f=0.00270465
BFGS iter 244: f=0.00270464
BFGS iter 245: f=0.00270463
BFGS iter 246: f=0.00270462
BFGS iter 247: f=0.00270456
BFGS iter 248: f=0.00270451
BFGS iter 249: f=0.00270438
BFGS iter 250: f=0.00269341
BFGS iter 251: f=0.00269311
BFGS iter 252: f=0.00269214
BFGS iter 253: f=0.00269143
BFGS iter 254: f=0.00269054
BFGS iter 255: f=0.0026873
BFGS iter 256: f=0.00268281
BFGS iter 257: f=0.00264812
BFGS iter 258: f=0.00263939
BFGS iter 259: f=0.00263423
BFGS iter 260: f=0.00262988
BFGS iter 261: f=0.0026211
BFGS iter 262: f=0.00261021
BFGS iter 263: f=0.00260734
BFGS iter 264: f=0.00260084
BFGS iter 265: f=0.00259552
BFGS iter 266: f=0.00259264
BFGS iter 267: f=0.00259131
BFGS iter 268: f=0.00259063
BFGS iter 269: f=0.00258824
BFGS iter 270: f=0.00257917
BFGS iter 271: f=0.00256994
BFGS iter 272: f=0.00256753
BFGS iter 273: f=0.0025657
BFGS iter 274: f=0.0025652
BFGS iter 275: f=0.00256474
BFGS iter 276: f=0.00256429
BFGS iter 277: f=0.00256338
BFGS iter 278: f=0.00255897
BFGS iter 279: f=0.00255802
BFGS iter 280: f=0.00255409
BFGS iter 281: f=0.00255221
BFGS iter 282: f=0.00255025
BFGS iter 283: f=0.00254862
BFGS iter 284: f=0.00254772
BFGS iter 285: f=0.00254648
BFGS iter 286: f=0.00254477
BFGS iter 287: f=0.00254173
BFGS iter 288: f=0.0025369
BFGS iter 289: f=0.00253585
BFGS iter 290: f=0.00253502
BFGS iter 291: f=0.00253463
BFGS iter 292: f=0.00253444
BFGS iter 293: f=0.00253432
BFGS iter 294: f=0.00253429
BFGS iter 295: f=0.0025342
BFGS iter 296: f=0.0025341
BFGS iter 297: f=0.00253404
BFGS iter 298: f=0.002534
BFGS iter 299: f=0.00253395
BFGS iter 300: f=0.00252322
BFGS iter 301: f=0.00252289
BFGS iter 302: f=0.00252235
BFGS iter 303: f=0.00252212
BFGS iter 304: f=0.00252156
BFGS iter 305: f=0.00251649
BFGS iter 306: f=0.00251583
BFGS iter 307: f=0.00251293
BFGS iter 308: f=0.00251111
BFGS iter 309: f=0.00250863
BFGS iter 310: f=0.00250779
BFGS iter 311: f=0.00250659
BFGS iter 312: f=0.00250586
BFGS iter 313: f=0.00250474
BFGS iter 314: f=0.00250387
BFGS iter 315: f=0.00250346
BFGS iter 316: f=0.00250318
BFGS iter 317: f=0.00250271
BFGS iter 318: f=0.00250213
BFGS iter 319: f=0.00250114
BFGS iter 320: f=0.00249971
BFGS iter 321: f=0.00249804
BFGS iter 322: f=0.00249692
BFGS iter 323: f=0.00249533
BFGS iter 324: f=0.00249372
BFGS iter 325: f=0.00249296
BFGS iter 326: f=0.00249198
BFGS iter 327: f=0.00249081
BFGS iter 328: f=0.00249064
BFGS iter 329: f=0.00249054
BFGS iter 330: f=0.00249051
BFGS iter 331: f=0.00249041
BFGS iter 332: f=0.00249017
BFGS iter 333: f=0.00249002
BFGS iter 334: f=0.00248992
BFGS iter 335: f=0.00248968
BFGS iter 336: f=0.00248671
BFGS iter 337: f=0.00248555
BFGS iter 338: f=0.00248535
BFGS iter 339: f=0.00248512
BFGS iter 340: f=0.00248474
BFGS iter 341: f=0.00248392
BFGS iter 342: f=0.00248054
BFGS iter 343: f=0.00247726
BFGS iter 344: f=0.00247566
BFGS iter 345: f=0.00247551
BFGS iter 346: f=0.00247543
BFGS iter 347: f=0.00247539
BFGS iter 348: f=0.00247534
BFGS iter 349: f=0.00247525
BFGS iter 350: f=0.00246231
BFGS iter 351: f=0.00246193
BFGS iter 352: f=0.00246093
BFGS iter 353: f=0.00246045
BFGS iter 354: f=0.00245989
BFGS iter 355: f=0.00245937
BFGS iter 356: f=0.00245925
BFGS iter 357: f=0.00245906
BFGS iter 358: f=0.00245885
BFGS iter 359: f=0.00245879
BFGS iter 360: f=0.00245863
BFGS iter 361: f=0.0024584
BFGS iter 362: f=0.00245821
BFGS iter 363: f=0.00245809
BFGS iter 364: f=0.00245797
BFGS iter 365: f=0.00245776
BFGS iter 366: f=0.0024568
BFGS iter 367: f=0.00245565
BFGS iter 368: f=0.00245394
BFGS iter 369: f=0.00245379
BFGS iter 370: f=0.00245375
BFGS iter 371: f=0.00245372
BFGS iter 372: f=0.00245369
BFGS iter 373: f=0.00245368
BFGS iter 374: f=0.00245366
BFGS iter 375: f=0.00245365
BFGS iter 376: f=0.00245365
BFGS iter 377: f=0.00245365
BFGS iter 378: f=0.00245363
BFGS iter 379: f=0.0024535
BFGS iter 380: f=0.00245287
BFGS iter 381: f=0.00245257
BFGS iter 382: f=0.0024525
BFGS iter 383: f=0.00245248
BFGS iter 384: f=0.00245248
BFGS iter 385: f=0.00245248
BFGS iter 386: f=0.00245248
BFGS iter 387: f=0.00245248
BFGS iter 388: f=0.00245241
BFGS iter 389: f=0.00245206
BFGS iter 390: f=0.00245184
BFGS iter 391: f=0.00245168
BFGS iter 392: f=0.00245166
BFGS iter 393: f=0.00245166
BFGS iter 394: f=0.00245166
BFGS iter 395: f=0.00245166
BFGS iter 396: f=0.00245166
BFGS iter 397: f=0.00245166
BFGS ended due to small decr. for 1 iteration
BFGS iter 398: f=0.00245166
MTPR training ended
Rescaling...
   scaling = 0.0149701866991442, condition number = 185.756243192798
   scaling = 0.0163311127627028, condition number = 174.928326668073
   scaling = 0.017964224038973, condition number = 161.854905322295
   scaling = 0.0197606464428703, condition number = 148.656589169526
   scaling = 0.0215570688467676, condition number = 137.035191678869
Rescaling to 0.0215570688467676... done
Rescaling...
   scaling = 0.017964224038973, condition number = 161.854905322295
   scaling = 0.0195973353152433, condition number = 149.792046001214
   scaling = 0.0215570688467676, condition number = 137.035191678869
   scaling = 0.0237127757314444, condition number = 125.051359871022
   scaling = 0.0258684826161212, condition number = 114.887994578996
Rescaling to 0.0258684826161212... done
Rescaling...
   scaling = 0.0215570688467676, condition number = 137.035191678869
   scaling = 0.023516802378292, condition number = 126.060213952015
   scaling = 0.0258684826161212, condition number = 114.887994578996
   scaling = 0.0284553308777333, condition number = 104.625413727234
   scaling = 0.0310421791393454, condition number = 96.02741035608
Rescaling to 0.0310421791393454... done
Rescaling...
   scaling = 0.0258684826161212, condition number = 114.887994578996
   scaling = 0.0282201628539504, condition number = 105.483329918254
   scaling = 0.0310421791393454, condition number = 96.02741035608
   scaling = 0.03414639705328, condition number = 87.4064884353505
   scaling = 0.0372506149672145, condition number = 80.2145227069915
Rescaling to 0.0372506149672145... done
Rescaling...
   scaling = 0.0310421791393454, condition number = 96.0274102693927
   scaling = 0.0338641954247404, condition number = 88.1254302376283
   scaling = 0.0372506149672145, condition number = 80.2145227069915
   scaling = 0.0409756764639359, condition number = 73.0221395251975
   scaling = 0.0447007379606574, condition number = 67.0322875899666
Rescaling to 0.0447007379606574... done
Rescaling...
   scaling = 0.0372506149672145, condition number = 80.2145227069915
   scaling = 0.0406370345096885, condition number = 73.6213813764353
   scaling = 0.0447007379606574, condition number = 67.0322875899666
   scaling = 0.0491708117567231, condition number = 61.0495123525712
   scaling = 0.0536408855527888, condition number = 56.072110662989
Rescaling to 0.0536408855527888... done
Rescaling...
   scaling = 0.0447007379606574, condition number = 67.0322875899666
   scaling = 0.0487644414116262, condition number = 61.5477144677796
   scaling = 0.0536408855527888, condition number = 56.072110662989
   scaling = 0.0590049741080677, condition number = 51.1052889038377
   scaling = 0.0643690626633466, condition number = 46.9771630925795
Rescaling to 0.0643690626633466... done
Rescaling...
   scaling = 0.0536408855527888, condition number = 56.072110662989
   scaling = 0.0585173296939515, condition number = 51.5187001851845
   scaling = 0.0643690626633466, condition number = 46.9771630925795
   scaling = 0.0708059689296813, condition number = 42.8622776698775
   scaling = 0.0772428751960159, condition number = 39.4465212900936
Rescaling to 0.0772428751960159... done
Rescaling...
   scaling = 0.0643690626633466, condition number = 46.9771630925795
   scaling = 0.0702207956327418, condition number = 43.2045817113164
   scaling = 0.0772428751960159, condition number = 39.4465212900936
   scaling = 0.0849671627156175, condition number = 36.0467784876653
   scaling = 0.0926914502352191, condition number = 33.2297023023908
Rescaling to 0.0926914502352191... done
Rescaling...
   scaling = 0.0772428751960159, condition number = 39.4465212900936
   scaling = 0.0842649547592901, condition number = 36.3293611019914
   scaling = 0.0926914502352191, condition number = 33.2297023023908
   scaling = 0.101960595258741, condition number = 30.4319342387173
   scaling = 0.111229740282263, condition number = 28.1198758950078
Rescaling to 0.111229740282263... done
Rescaling...
   scaling = 0.0926914502352191, condition number = 33.2297023023908
   scaling = 0.101117945711148, condition number = 30.6642014382126
   scaling = 0.111229740282263, condition number = 28.1198758950078
   scaling = 0.122352714310489, condition number = 24.6863295585745
   scaling = 0.133475688338716, condition number = 20.9794058675843
Rescaling to 0.133475688338716... done
Rescaling...
   scaling = 0.111229740282263, condition number = 28.1198758950078
   scaling = 0.121341534853378, condition number = 25.0747846510591
   scaling = 0.133475688338716, condition number = 20.9794058675843
   scaling = 0.146823257172587, condition number = 17.5959156810632
   scaling = 0.160170826006459, condition number = 15.0242485817043
Rescaling to 0.160170826006459... done
Rescaling...
   scaling = 0.133475688338716, condition number = 20.9794058675843
   scaling = 0.145609841824053, condition number = 17.86549626059
   scaling = 0.160170826006459, condition number = 15.0242485817043
   scaling = 0.176187908607105, condition number = 12.1571789017352
   scaling = 0.19220499120775, condition number = 9.58048426809286
Rescaling to 0.19220499120775... done
Rescaling...
   scaling = 0.160170826006459, condition number = 15.0242485817043
   scaling = 0.174731810188864, condition number = 12.4389852439757
   scaling = 0.19220499120775, condition number = 9.58048426809286
   scaling = 0.211425490328525, condition number = 7.41756823726473
   scaling = 0.2306459894493, condition number = 5.90582477543082
Rescaling to 0.2306459894493... done
Rescaling...
   scaling = 0.19220499120775, condition number = 9.58048426809286
   scaling = 0.209678172226637, condition number = 7.58287190722809
   scaling = 0.2306459894493, condition number = 5.90582477543082
   scaling = 0.25371058839423, condition number = 4.63731046737141
   scaling = 0.27677518733916, condition number = 3.75244600061754
Rescaling to 0.27677518733916... done
Rescaling...
   scaling = 0.2306459894493, condition number = 5.90582477543082
   scaling = 0.251613806671964, condition number = 4.73419527629629
   scaling = 0.27677518733916, condition number = 3.75244600061754
   scaling = 0.304452706073077, condition number = 3.01325687050123
   scaling = 0.332130224806993, condition number = 2.50170240960672
Rescaling to 0.332130224806993... done
Rescaling...
   scaling = 0.27677518733916, condition number = 3.75244600061754
   scaling = 0.301936568006357, condition number = 3.0695275820275
   scaling = 0.332130224806993, condition number = 2.50170240960672
   scaling = 0.365343247287692, condition number = 2.07959029049435
   scaling = 0.398556269768391, condition number = 1.93350843833805
Rescaling to 0.398556269768391... done
Rescaling...
   scaling = 0.332130224806993, condition number = 2.50170240960672
   scaling = 0.362323881607628, condition number = 2.1114735534412
   scaling = 0.398556269768391, condition number = 1.93350843833805
   scaling = 0.43841189674523, condition number = 2.24059854296408
   scaling = 0.478267523722069, condition number = 2.6220659145479
Rescaling to 0.398556269768391... done

		* * * TRAIN ERRORS * * *

_________________Errors report_________________
Energy:
	Errors checked for 100 configurations
	Maximal absolute difference = 0.021412
	Average absolute difference = 0.0103834
	RMS     absolute difference = 0.0121462

Energy per atom:
	Errors checked for 100 configurations
	Maximal absolute difference = 0.000669125
	Average absolute difference = 0.000324481
	RMS     absolute difference = 0.000379569

Forces:
	Errors checked for 3200 atoms
	Maximal absolute difference = 0.228674
	Average absolute difference = 0.0254642
	RMS     absolute difference = 0.0485901
	Max(ForceDiff) / Max(Force) = 0.146463
	RMS(ForceDiff) / RMS(Force) = 0.136248

Stresses (in eV):
	Errors checked for 100 configurations
	Maximal absolute difference = 4.3007
	Average absolute difference = 0.240868
	RMS     absolute difference = 0.798759
	Max(StresDiff) / Max(Stres) = 0.336906
	RMS(StresDiff) / RMS(Stres) = 0.215108

Stresses (in GPa):
	Errors checked for 100 configurations
	Maximal absolute difference = 1.30602
	Average absolute difference = 0.073146
	RMS     absolute difference = 0.242565
	Max(StresDiff) / Max(Stres) = 0.336906
	RMS(StresDiff) / RMS(Stres) = 0.215108
_______________________________________________


		* * * VALIDATION ERRORS * * *

_________________Errors report_________________
Energy:
	Errors checked for 1000 configurations
	Maximal absolute difference = 0.0796663
	Average absolute difference = 0.0330523
	RMS     absolute difference = 0.0388376

Energy per atom:
	Errors checked for 1000 configurations
	Maximal absolute difference = 0.00248957
	Average absolute difference = 0.00103289
	RMS     absolute difference = 0.00121367

Forces:
	Errors checked for 32000 atoms
	Maximal absolute difference = 0.209995
	Average absolute difference = 0.0256927
	RMS     absolute difference = 0.0483202
	Max(ForceDiff) / Max(Force) = 0.117663
	RMS(ForceDiff) / RMS(Force) = 0.138185

Stresses (in eV):
	Errors checked for 1000 configurations
	Maximal absolute difference = 3.29113
	Average absolute difference = 0.187755
	RMS     absolute difference = 0.59607
	Max(StresDiff) / Max(Stres) = 0.224965
	RMS(StresDiff) / RMS(Stres) = 0.157132

Stresses (in GPa):
	Errors checked for 1000 configurations
	Maximal absolute difference = 0.99944
	Average absolute difference = 0.057017
	RMS     absolute difference = 0.181013
	Max(StresDiff) / Max(Stres) = 0.224965
	RMS(StresDiff) / RMS(Stres) = 0.157132
_______________________________________________

